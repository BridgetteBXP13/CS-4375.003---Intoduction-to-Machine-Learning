---
title: "Classification"
student: "Isabelle Kirby", "Bridgette Bryant" 
NetID: "IRK180000", "BMB180001"
Assignment: "Linear Models"
Date: "09/16/2021"
---

Despite its name, in logistic regression we are classifying data. This means that the target variable is qualitative and we're trying to discern what class an observation is in. As such, linear models for logistic regression create decision boundaries to split observations into regions populated by mostly one classification. This is computationally inexpensive and works well when data can be separated cleanly (linearly). It also displays the probabilistic output in an manageable manner. This being said, a model is only as good as the data that is presented to it. When given a data set that is too small or unbalanced, the model itself isn't able to be trained well enough to be reliable for use in professional settings.It also tends to under fit data as it's not complex enough to capture non-linear decision boundaries. 
................

Loading in file
```{r}
df <- read.csv("covtype.csv")
df$Cover_Type <- factor(df$Cover_Type)
```

Now we are creating the training and testing sets (80% train, 20% test).
```{r}
i <- sample(1:nrow(df), nrow(df)*.80, replace=FALSE)
train <- df[i,]
test <- df[-i,]

```
Data Exploration
```{r}
head(train)
tail(train)

median(train$class)

mode(train$cap.color)

str(train)
```
Plots
```{r}
hist(train$Elevation)
hist(train$Soil_Type)
slices <- c(sum(train$Wilderness_Area1==1, na.rm=TRUE), sum(train$Wilderness_Area2==1, na.rm=TRUE), sum(train$Wilderness_Area3==1, na.rm=TRUE), sum(train$Wilderness_Area4==1, na.rm=TRUE))

labls <- c("Wilds 1","Wilds 2","Wilds 3","Wilds 4")
pie(slices,labels = labls, main="Types of Wild Area", col=c("green", "blue", "yellow", "red"))

```
Logistic Regression
```{r}
tree_type1 <- df
tree_type1$Cover_Type <- as.factor(ifelse (tree_type1$Cover_Type=="1",1,0))

tree_type2 <- df
tree_type2$Cover_Type <- as.factor(ifelse (tree_type2$Cover_Type=="2",1,0))

tree_type3 <- df
tree_type3$Cover_Type <- as.factor(ifelse (tree_type3$Cover_Type=="3",1,0))

tree_type4 <- df
tree_type4$Cover_Type <- as.factor(ifelse (tree_type4$Cover_Type=="4",1,0))

tree_type5 <- df
tree_type5$Cover_Type <- as.factor(ifelse (tree_type5$Cover_Type=="5",1,0))

tree_type6 <- df
tree_type6$Cover_Type <- as.factor(ifelse (tree_type6$Cover_Type=="6",1,0))

tree_type7 <- df
tree_type7$Cover_Type <- as.factor(ifelse (tree_type7$Cover_Type=="7",1,0))

fun <- function(dataf, i){
  train <- dataf[i,]
  test <- dataf[-i,]
  glm1 <- glm(Cover_Type~., data=train, family="binomial")
  probs <- predict(glm1, newdata=test)
  pred <- ifelse(probs>0.5, 1, 0)
  acc <- mean(pred==test$Cover_Type)
  print(paste("accuracy = ", acc))
  table(pred, test$Cover_Type)
}

fun(tree_type1, i)
fun(tree_type2, i)
fun(tree_type3, i)
fun(tree_type4, i)
fun(tree_type5, i)
fun(tree_type6, i)
fun(tree_type7, i)


#glm1 <- glm(Slope~Elevation, data=train, family=poisson())
#summary(glm1)
```


Naive Bayes
```{r}
library(e1071)
nb1 <- naiveBayes(Slope~., data=train)
nb1
```

