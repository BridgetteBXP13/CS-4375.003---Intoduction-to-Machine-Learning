---
title: "Linear Models: Classification"
author: "Isabelle Kirby, Bridgette Bryant"
output:
  pdf_document: default
  html_document: default
---

Logistic Regression:

Despite its name, in logistic regression we are classifying data. This means that the target variable is qualitative and we're trying to discern what class an observation is in. As such, linear models for logistic regression create decision boundaries to split observations into regions populated by mostly one classification. This is computationally inexpensive and works well when data can be separated cleanly (linearly). It also displays the probabilistic output in an manageable manner. This being said, a model is only as good as the data that is presented to it. When given a data set that is too small or unbalanced, the model itself isn't able to be trained well enough to be reliable for use in professional settings.It also tends to under fit data as it's not complex enough to capture non-linear decision boundaries. 
................

Loading in file
```{r}
df <- read.csv("covtype.csv")
df$Cover_Type <- factor(df$Cover_Type)
```

Now we are creating the training and testing sets (80% train, 20% test).
```{r}
i <- sample(1:nrow(df), nrow(df)*.80, replace=FALSE)
train <- df[i,]
test <- df[-i,]

```
Data Exploration
```{r}

head(train)
tail(train)

median(train$Elevation)

getmode <- function(d) {
  uniqd <- unique(d)
  uniqd[which.max(tabulate(match(d, uniqd)))]
}
getmode(train$Cover_Type)
getmode(train$Soil_Type)

str(train)
```
Plots
```{r}
hist(train$Elevation)
hist(train$Soil_Type)

counts <- table(train$Cover_Type)
barplot(counts)

slices <- c(sum(train$Wilderness_Area1==1, na.rm=TRUE), sum(train$Wilderness_Area2==1, na.rm=TRUE), sum(train$Wilderness_Area3==1, na.rm=TRUE), sum(train$Wilderness_Area4==1, na.rm=TRUE))

labls <- c("Wilds 1","Wilds 2","Wilds 3","Wilds 4")
pie(slices,labels = labls, main="Types of Wild Area", col=c("green", "blue", "yellow", "red"))

```
Explanation of Logistic Regression below:

Our logistical regression has relatively high accuracy rates for all of the different types of trees. Since there are seven different types of trees present in the data, we split the data solely about the tree type into a dataframe specifically made for them. From there we made models for each tree type.

Type 1:
For this model we had about a 75% accuracy level. This model did a very good job at identifying whether or not a tree was apart of tree type 1. That being said it identified a lot more trees as tree type 1 that were not in fact in that tree type than false negatives. This leads me to believe that this might be a more common tree type with less niche demands for growth, allowing for the model to falsely think that some trees are of that type.

Type 2:
For this model we had an accuracy level of about 73%, meaning that our model was mostly accurate. It also had more difficulty with false positives than it did with false negatives. This again may be due to that tree having less specific needs for its growth and having a high variance in the data.

Type 3:
This model had an accuracy level of about 96%, a truly impressive accuracy level. It also had a more difficult time with identifying more false positives than false negatives, but that could be due to the common trees in type 1 and type 2. This tree most likely has some specific needs (ie: a lower variance in the data). This does mean that the bias is higher in this model, however it is still incredibly accurate.

Type 4:
This model had an accuracy of 99.6%. It identified a lot of trees as correctly being apart of that tree type. It seems that it had more of an issue with false positives than it did false negatives, but it still accurately described the data set given.

Type 5:
For this model we have a 98% accuracy level. Oddly enough, for this tree type its saying that there were no true negatives. That would mean that there were only 1855 trees that aren't of type five (this is from the false positive cell). This is a high number of trees of type 5.

Type 6:
For this model we have nearly a 97% accuracy level. It is also identified more true positives than anything else, meaning that the data used was filled mostly with type 6 trees. It also had a larger amount of false positives than false negatives due to the more resilient and less specific trees in the data set.

Type 7:
This model had around a 98% accuracy level. As with the other models its reporting a lot of true positives, as well as false positives. This one however has more true negatives than the other models did. This tree type probably had more specific needs than other trees, making it easy to find what trees are not that type.

In conclusion, despite the accuracy levels of each regression being high there seems to be an unusually high number of true positives for each tree type. We are unsure the cause of this as our function is modeled very closely to the one used in class.
```{r}
tree_type1 <- df
tree_type1$Cover_Type <- as.factor(ifelse (tree_type1$Cover_Type=="1",1,0))

tree_type2 <- df
tree_type2$Cover_Type <- as.factor(ifelse (tree_type2$Cover_Type=="2",1,0))

tree_type3 <- df
tree_type3$Cover_Type <- as.factor(ifelse (tree_type3$Cover_Type=="3",1,0))

tree_type4 <- df
tree_type4$Cover_Type <- as.factor(ifelse (tree_type4$Cover_Type=="4",1,0))

tree_type5 <- df
tree_type5$Cover_Type <- as.factor(ifelse (tree_type5$Cover_Type=="5",1,0))

tree_type6 <- df
tree_type6$Cover_Type <- as.factor(ifelse (tree_type6$Cover_Type=="6",1,0))

tree_type7 <- df
tree_type7$Cover_Type <- as.factor(ifelse (tree_type7$Cover_Type=="7",1,0))

fun <- function(dataf, i){
  funtrain <- dataf[i,]
  funtest <- dataf[-i,]
  glm1 <- glm(Cover_Type~., data=funtrain, family="binomial")
  probs <- predict(glm1, newdata=funtest)
  pred <- ifelse(probs>0.5, 1, 0)
  acc <- mean(pred==funtest$Cover_Type)
  print(paste("accuracy = ", acc))
  table(pred, funtest$Cover_Type)
}

fun(tree_type1, i)
fun(tree_type2, i)
fun(tree_type3, i)
fun(tree_type4, i)
fun(tree_type5, i)
fun(tree_type6, i)
fun(tree_type7, i)

```

Naive Bayes, Building the model
```{r}
library(e1071)
nb1 <- naiveBayes(Cover_Type~., data=train)
nb1
```

Evaluate on the test data
```{r}
pbayes <- predict(nb1, newdata=test, type="class")
table(pbayes, test$Cover_Type)
```
```{r}
getmode(pbayes==test$Cover_Type)
```

