---
title: "Regression"
student: "Isabelle Kirby", "Bridgette Bryant" 
NetID: "IRK180000", "BMB180001"
Assignment: "Linear Models"
Date: "09/16/2021"
---
Linear Regression:

In simple terms, linear regression tries to predict a quantitative target by fitting a linear model to the input of the predictors to best fit the target in the training data. A simple linear regression model is often in the form of y = wX + b where y values as the targets, X values as the predictors, the linear model creates the weight matrix w and the fitting parameter b. Linear models don't always have to be straight lines, they can be complex polynomials with multiple predictor variables. Once a linear regression model is created/trained, you can judge it's performance by the standard error, t-value, and most of all the p-value. A p-value very close to 0 is the most ideal, R will show you with '***' to ' ' next to the value with '***' showing an excellent p-value. Residual standard error is also another way to measure your linear model, it calculated from residual sum of squared errors, and measures the lack of fit the model has for the data. However, it is in the units of the data and can be difficult to interpret at times, therefore it is better to use r-squared which lies between 0 and 1. The closer to 1 the better the model. You can also evaluate the model with correlation, which shows the correlation, which shows the degree in terms of 1 and -1, a close to 1 shows it is strongly positively correlation, a -1 shows it is strongly negatively correlation, and lastly a 0 shows no correlation. Linear regression has a few weaknesses including iteration effects, where there is too much synergy between predictors, causing them to have too much impact as a whole on the model. Along with confounding variables, which correlate with both the target and predictors which can give too high of a correlation in the training data. Also, hidden variables in the model, this can lead to false conclusions and assumptions such as a correlation being a causation in the data. Linear regression is also very likely to underfit data, causing it to fail at capturing some of the data and have a low accuracy. However, linear regression is very simple to implement and has a very high performance on linearly separable data sets. Also, overfitting data is unlikely to happen and can be very easy to fix in linear models using regularization.

We chose the "Air_Pollution.csv" for this assignment. It contains data of the overall air quality of different cities overtime. 

It has 32191 observations initially.
```{r}
df <- read.csv("Air_Pollution.csv")
str(df)
```
We then fill incomplete rows cells with the medians from their rows
```{r}
df$PM2.5[is.na(df$PM2.5)] <- mean(df$PM2.5, na.rm=TRUE)
df$PM10[is.na(df$PM10)] <- mean(df$PM10, na.rm=TRUE)
df$NO2[is.na(df$NO2)] <- mean(df$NO2, na.rm=TRUE)
df$PM25.temporal.coverage[is.na(df$PM25.temporal.coverage)] <- mean(df$PM25.temporal.coverage, na.rm=TRUE)
df$PM10.temporal.coverage[is.na(df$PM10.temporal.coverage)] <- mean(df$PM10.temporal.coverage, na.rm=TRUE)
df$NO2.temporal.coverage[is.na(df$NO2.temporal.coverage)] <- mean(df$NO2.temporal.coverage, na.rm=TRUE)

str(df)
```

Now we are creating the training and testing sets (80% train, 20% test).
```{r}
i <-sample(1:nrow(df), nrow(df)*0.80, replace=FALSE)
train <- df[i,]
test <- df[-i,]
```

```{r}
head(train)

tail(train)

range(train$PM2.5, na.rm=FALSE)
range(train$PM10, na.rm=FALSE)

mean(train$PM2.5)
mean(train$PM10)

median(train$PM2.5)
median(train$PM10)

```
These are our plots
```{r}
hist(train$PM2.5)
plot(train$Year, train$NO2)
```



Plotting the linear regression
```{r}
lm1 <- lm(NO2~PM2.5, data=train)
summary(lm1)
```


```{r}
plot(lm1)
```

The residual plots above are summarized below:

Residuals vs Fitted:
In this plot it shows whether the linearity holds, indicated by the mean residual value for every fitted value region being close to 0 (the red line being close to the dashed line). As you can see the redline is very close to the dashed line close to 20, but drifts downwards as we get close to 40 and 0. We can also see that we may have outliers at point 16687 and the other two below (I cannot read them because of the way R printed them ontop of eachother). Overall it shows that our model is a good model. 

Normal Q-Q:
This graph shows the standardized residuals and theoretical quantiles, if they come from the same distribution then the points should form a roughly straight line. Our data set has a very straight line until around (4, 10) where we once again see our outlier points 16687 and the couple below. As you can see it once again steers away from the gray line in a strong curve shape. Although the line is still fairly straight overall but we may have some skewed data in our dataset (possibly from having to replace many missing values with the mean). It can also mean since it is curved at the edge that our dataset contains more extreme values.

Scale Location:
Our scale-location graph is very uneven and is basically a black blob. The residuals are supposed to be spread evenly across the range of predictors, which can show the assumption of equal variance. As you can tell ours is definately not good, this shows that our assumption of equal variance is very incorrect. 


Leverage vs Residuals:
In this plot the leverage is the extent to which the coefficients in the regression model would change if a particular observation was removed from the dataset. Thus observations with high leverage have strong influence on the coefficients of the regression model. The standardized residuals refer to the standardized difference between a predicted value and actual value of an observation. You can tell in the plots above that we have no influential points as all the points of the data are within the cook's distance (dotted lines).

```{r}
lm2 <- lm(NO2~PM2.5+PM10, data=train)
summary(lm2)
```
```{r}
plot(lm2)
```


```{r}
lm3 <- lm(PM2.5~Year+NO2, data=train)
summary(lm3)
```
```{r}
plot(lm3)
```


